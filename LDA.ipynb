{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "\n",
    "from collections import Counter\n",
    "from pyspark.ml.clustering import LDA, LDAModel, DistributedLDAModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "DATA_FOLDER = \"data/\"\n",
    "RESULTS_FOLDER = \"results/\"\n",
    "RDD_FILENAME = \"stemstem.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading stemmed corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the corpus of stemmed reviews. Stemming will prevent us from getting noise in our analysis from words having the same stem but being considered as different words (eg. wait, waited, waiting, ...).\n",
    "\n",
    "Unfortunately, for various performance reasons and to be able to perform all computations in a reasonable amount of time, we first limit ourselves to a corpus of 50k reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rdd(filename, max_size = None):\n",
    "    df = spark.read.parquet(DATA_FOLDER + filename)\n",
    "    rdd = df.rdd.zipWithIndex().map(lambda r: (r[1], r[0][\"Words\"]))\n",
    "    return rdd if max_size == None else sc.parallelize(rdd.take(max_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = get_rdd(RDD_FILENAME, max_size = 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the `LDAModels` provided by `pyspark`, we need to shape the data. More precisely, we need to compute the number of occurrences of each term in each review. This is the purpose of the function `count_words` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(rdd):\n",
    "    def word_occurrences(l):\n",
    "        counter = dict(Counter(l))\n",
    "        return list(zip(counter.keys(), counter.values()))\n",
    "    \n",
    "    return rdd.mapValues(word_occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we establish the set of all words present in our reviews. This is done in the `wordify` function below. `words` is constructed from iteratively taking the union of the set of words of each review. Finally, we assign to each word an unique integer index. This is going to be useful later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordify(rdd):\n",
    "    words = list(rdd.map(lambda p: set(p[1])).reduce(lambda x, y: x | y))\n",
    "    return dict([(words[i], i) for i in range(len(words))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of data preparation is to map the review of each entry of `rdd` to a vector of size `len(words)` containing at index $i$ the number of occurrences of word $i$. Since each review contains only a small subset of all possible words, we naturally choose a `SparseVector` instance for obvious performance reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDAify(rdd, words):\n",
    "    return rdd.mapValues(lambda l: [(words[w], occurrences) for (w, occurrences) in l]).mapValues(lambda l: Vectors.sparse(len(words), dict(l))).map(lambda p: list(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words can be computed in the following manner: `words = wordify(rdd)`. Nevertheless, since this computation is quite time consuming, we decided to do it once for all and to store the result in Pickle file. Hence, we load the dictionary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "with open(RESULTS_FOLDER + \"words_lda.pickle\", 'rb') as handle:\n",
    "    words = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we create the data matrix X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = LDAify(count_words(rdd), words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core LDA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA's `describeTopics()` method returns a list of `k` topics assigning to each one a word distribution. However, here words indices are used and not words themselves. Therefore, we implement the following `format_terms` function to help us retrieving a word from its index in `words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_terms(indices, weights):\n",
    "    words_list = list(words)\n",
    "    return [(words_list[index], weights[i]) for (i, index) in enumerate(indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it's worth noticing that LDA depends on two hyperparameters $\\alpha$ and $\\beta$, describing respectively the document and term distributions. We need to tune those parameters according to our needs and we do this using a simple grid search.\n",
    "\n",
    "This is what does the `test_coefs` function below. Also, it prints the topics created by the LDA so that we can manually evaluate their relevance to find the best values for $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_coefs():\n",
    "    res = []\n",
    "    for alpha in [1.01, 1.5, 2, 3]:\n",
    "        for beta in [1.01, 1.5, 2, 3]:\n",
    "            print(\"alpha = \" + str(alpha) + \" / beta = \" + str(beta))\n",
    "            lda = LDA(k = 10).setFeaturesCol(\"feature\").setMaxIter(100).setOptimizer(\"em\").setDocConcentration([float(alpha)]).setTopicConcentration(float(beta))\n",
    "            X_ = X.toDF([\"id\", \"feature\"])\n",
    "            model = lda.fit(X_)\n",
    "            res += [model.describeTopics().rdd.map(lambda r: (r[0], format_terms(r[1], r[2]))).take(10)]\n",
    "    \n",
    "    interesting_indices = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k, (_, words) in enumerate(res[4*i + j]):\n",
    "                if \"kindl\" in list(map(lambda p: p[0], words)):\n",
    "                    interesting_indices.append((4*i+j, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen from above\n",
    "alpha = 2\n",
    "beta = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, training a LDA model is quite time consuming. Therefore, we saved the model for the values $\\alpha = 1$ and $\\beta = 1.5$ in a Parquet file. Retrieving the model can be done as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = DistributedLDAModel.load(RESULTS_FOLDER + \"kindle_lda_model.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topics extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "    review: 1.669342772819849%\n",
      "    relationship: 1.227819401281716%\n",
      "    togeth: 1.1226048807265143%\n",
      "    into: 0.9942960889184277%\n",
      "    onli: 0.9100196149317308%\n",
      "    sexi: 0.8871170059162949%\n",
      "    she': 0.8252354260346444%\n",
      "    doesn't: 0.7530020812918945%\n",
      "    romanc: 0.7046308878346287%\n",
      "    passion: 0.5523344025810424%\n",
      "\n",
      "Topic 2\n",
      "    novel: 3.416132220056701%\n",
      "    mysteri: 3.056578227312123%\n",
      "    twist: 2.247111886674283%\n",
      "    suspens: 1.6679165260551003%\n",
      "    excel: 1.6058436532190203%\n",
      "    fiction: 1.342177967207062%\n",
      "    adventur: 1.2226281607025942%\n",
      "    murder: 1.1559497799029421%\n",
      "    humor: 1.138142625814316%\n",
      "    action: 0.9842488594565195%\n",
      "\n",
      "Topic 3\n",
      "    seri: 7.396401356429165%\n",
      "    becaus: 5.167379252584321%\n",
      "    author: 4.1938369866492335%\n",
      "    didn't: 3.359057931263465%\n",
      "    definit: 2.6682958127516434%\n",
      "    anoth: 1.8695779456988404%\n",
      "    onli: 1.6990315579187822%\n",
      "    couldn't: 1.6985199654105725%\n",
      "    wasn't: 1.6225247585990297%\n",
      "    disappoint: 1.4891925098047047%\n",
      "\n",
      "Topic 4\n",
      "    stori: 24.13733218636862%\n",
      "    veri: 13.017869695957318%\n",
      "    enjoy: 10.444317898409237%\n",
      "    realli: 9.958972710773846%\n",
      "    recommend: 5.357020154731531%\n",
      "    littl: 4.765993762500799%\n",
      "    nice: 2.193206484147389%\n",
      "    highli: 1.973553738057103%\n",
      "    anyon: 1.497580712747966%\n",
      "    funni: 1.2131688413462027%\n",
      "\n",
      "Topic 5\n",
      "    famili: 2.5475465671776814%\n",
      "    everi: 2.478906673827554%\n",
      "    alway: 2.4575039233391562%\n",
      "    peopl: 2.2524309454613447%\n",
      "    happi: 1.3430531129595868%\n",
      "    everyon: 1.2906349338901972%\n",
      "    beauti: 1.260775663027364%\n",
      "    away: 1.2437774689471053%\n",
      "    emot: 1.1176326825589522%\n",
      "    someon: 1.0368597686157324%\n",
      "\n",
      "Topic 6\n",
      "    kindl: 3.4851631644532497%\n",
      "    easi: 3.1347912851666506%\n",
      "    worth: 2.2031098890168312%\n",
      "    inform: 2.101707277234647%\n",
      "    recip: 1.4500934092009663%\n",
      "    edit: 1.2264422253445908%\n",
      "    purchas: 1.1451439387904845%\n",
      "    simpl: 1.1191543504478274%\n",
      "    price: 1.0908632820561763%\n",
      "    error: 0.907890172640072%\n",
      "\n",
      "Topic 7\n",
      "    vampir: 1.1504778828937832%\n",
      "    magic: 0.993768653922828%\n",
      "    onli: 0.9475194464188865%\n",
      "    tale: 0.9119911057497841%\n",
      "    into: 0.7379412586360928%\n",
      "    becom: 0.6391268411779106%\n",
      "    evil: 0.6035439353139294%\n",
      "    dragon: 0.5272237515241361%\n",
      "    discov: 0.527087217235157%\n",
      "    forc: 0.520361319889943%\n",
      "\n",
      "Topic 8\n",
      "    charact: 7.916609945957475%\n",
      "    stori: 3.594943391212361%\n",
      "    reader: 2.3783095049921688%\n",
      "    into: 2.098875026784556%\n",
      "    author: 1.9831345959347724%\n",
      "    plot: 1.8636880414413062%\n",
      "    romanc: 1.6224680971677397%\n",
      "    scene: 1.1354941044048612%\n",
      "    origin: 1.0590461834987939%\n",
      "    review: 0.9305601102007053%\n",
      "\n",
      "Topic 9\n",
      "    mani: 1.393466337031174%\n",
      "    author: 1.2858657692254476%\n",
      "    understand: 1.1108965844667746%\n",
      "    peopl: 0.8453844245874245%\n",
      "    histori: 0.7528662945217954%\n",
      "    experi: 0.7524499625790269%\n",
      "    provid: 0.6337621058969057%\n",
      "    import: 0.6065805632512865%\n",
      "    explain: 0.5723827834142591%\n",
      "    into: 0.5668916533328499%\n",
      "\n",
      "Topic 10\n",
      "    can't: 10.611228797169789%\n",
      "    amaz: 4.718290448975991%\n",
      "    excit: 3.380731389981123%\n",
      "    awesom: 2.9894766053689388%\n",
      "    jack: 1.865220437126645%\n",
      "    zombi: 1.0136067964718347%\n",
      "    david: 0.9745608895211382%\n",
      "    super: 0.9134112272977353%\n",
      "    till: 0.6307585033674608%\n",
      "    chri: 0.6198056802175024%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_topics(model, dictionary):\n",
    "    words_list = list(dictionary)\n",
    "    for i, words in model.describeTopics(10).rdd.map(lambda r: (r[0], (zip(r[1], r[2])))).groupByKey().sortByKey().collect():\n",
    "        print(\"Topic \" + str(i+1))\n",
    "        for word_indices in words:\n",
    "            for word, per in (map(lambda w: (words_list[w[0]], w[1]), word_indices)):\n",
    "                print(\"    \" + word + \": \" + str(100*per) + \"%\")\n",
    "        print()\n",
    "\n",
    "print_topics(best_model, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the topics displayed above, we note that a large number of them seem related to specific genres that we might find in books. For example, the topic containing \"relationship\", \"together\", \"sexi\", \"romanc\", \"passion\" may be a genre that we could call \"Love & Romance\".\n",
    "\n",
    "However, a specific topic has retained our whole attention. It is topic number 6, essentially containing words that are really specific to Kindle features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kindle_topic = best_model.describeTopics(best_model.vocabSize()).rdd.filter(lambda r: r[0] == 5).map(lambda r: r[2]).collect()[0]\n",
    "kindle_topic_vect = Vectors.dense(kindle_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we are naturally interested in is finding a measure of similarity between reviews and this topic. This will give us a certain confidence on how much a review matches our Kindle topic. For this task, we chose a simply cosine similarity, widely used in topic extraction.\n",
    "\n",
    "Then we tune the desired similarity in order to select a decent number (~50) of best reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(u, v):\n",
    "    return 1 - u.dot(v)/(u.norm(2)*v.norm(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    return Vectors.sparse(v.size, dict(zip(v.indices, map(lambda x: x/len(words), v.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_reviews = X.map(lambda r: (r[0], sim(normalize(r[1]), kindle_topic_vect))).filter(lambda r: r[1] > 0.999998).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we retrieve the `ASIN` of the product those reviews concern, for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_asins = spark.read.parquet(\"data/\" + RDD_FILENAME).rdd.zipWithIndex().filter(lambda r: r[1] in matching_reviews.keys()).map(lambda r: r[0][\"Asin\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B00HMQVE4G',\n",
       " 'B00GR4XFPU',\n",
       " 'B00HUS1CCU',\n",
       " 'B005FCVU02',\n",
       " 'B0079UAT0A',\n",
       " 'B00K9XXA32',\n",
       " 'B005H4V4M2',\n",
       " 'B009IQNMTS',\n",
       " 'B00L19SVQ4',\n",
       " 'B00C8EUHQS',\n",
       " 'B00ASDAWUM',\n",
       " 'B00FS32GZG',\n",
       " 'B00BI4J0S0',\n",
       " 'B00JVF40C4',\n",
       " 'B008C9INNC',\n",
       " 'B00B392XIE',\n",
       " 'B004QGY35W',\n",
       " 'B00AXBK8YY',\n",
       " 'B00BPSJ0JI',\n",
       " 'B005DA0LN8',\n",
       " 'B005COO1X6',\n",
       " 'B00AFEOBZ6',\n",
       " 'B00B2K1AWO',\n",
       " 'B00I214D70',\n",
       " 'B00K31DCD8',\n",
       " 'B00AYYOSS2',\n",
       " 'B00B4ISWTS',\n",
       " 'B00DMNDTR8',\n",
       " 'B0073VIZB0',\n",
       " 'B00BC6SVY8',\n",
       " 'B00DR0B31U',\n",
       " 'B00ILKTBSI',\n",
       " 'B00BEMD8FW',\n",
       " 'B005IGBHRG',\n",
       " 'B00AV2FS36',\n",
       " 'B00BNVGUPO',\n",
       " 'B002RI9TIW',\n",
       " 'B00A94QQCI']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_asins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
